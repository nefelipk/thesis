\documentclass[12pt,a4paper]{report}
\usepackage[english]{babel}
\usepackage[utf8]{inputenc}

\usepackage[colorlinks]{hyperref}
\usepackage[colorinlistoftodos]{todonotes}

\usepackage[numbers]{natbib}

\usepackage[autostyle]{csquotes}
\usepackage[nottoc]{tocbibind}
\usepackage{indentfirst}

\usepackage[normalem]{ulem}
\usepackage{amsmath}

\newcommand{\defn}[1]{\enquote{\textit{#1}}}
\newcommand{\alleg}{\enquote}
\newcommand{\term}{\textit}
\newcommand{\acronym}{\MakeUppercase}
\newcommand{\itfrac}[2]{\frac{\textit{#1}}{\textit{#2}}}

\begin{document}
	{
		\hypersetup{linkcolor=black}
		\tableofcontents
	}
	
	\chapter{Introduction}
	\label{sec:intro}
	
	During the last years the focus of research for robotic applications evolved 
	from well structured indoor environments to unstructured outdoor environments. 
	With this expansion of interest, it is a crucial prerequisite to reliably 
	classify traversable ground in the environment, especially when it comes to 
	truly autonomous/self-supervised systems. This topic is typically referred to as 
	\term{traversability analysis} or \term{obstacle detection} \citep{Suger}. The 
	verb traverse is defined as \defn{to pass or move over, along, or through}. 
	Hence \term{traversability} refers to the affordance of being able to traverse 
	\citep{Ugur}. Failing on this task can cause great damage or restrict the robots 
	movement unnecessarily.
	\\
	
	So, traversability is the generic capability of a robotic ground 
	vehicle to navigate within environments of varying complexity, while ensuring 
	safety in terms of collisions or reaching unrecoverable states and achieving 
	goals in an optimal mode of operation \citep{Papadakis}. Occasionally other 
	terms such as \term{mobility} \citep{Lalonde}, \term{drivability} \citep{Droeschel}, 
	etc are used to describe the same concept.
	\\
	
	Although traversability is considered a fundamental capability for mobile 
	robots, in some cases it is limited to the problem of simple obstacle avoidance 
	\citep{Ugur}. When such approaches are used, the robot tries to avoid making any 
	physical contact with the environment, and heads only to open spaces. Its 
	response would be the same whether it encounters an impenetrable wall or a 
	balloon that can just be pushed aside without any damage. Therefore, methods 
	that can automatically learn the traversability affordances from the 
	robot’s interactions with the environment are valuable for robotics.
	\\
		
	In addition, there is the possibility that previously learned behavior is not 
	relevant, because the visual appearance and traversability of roads may 
	have changed due to various reasons \citep{Wigness}. That is probably why 
	geometry-based analysis is the direction followed by the majority of 
	traversability analysis methodologies in the past \citep{Papadakis}.
	\\
	
	Supervised learning approaches are unlikely to work reliably in unknown or 
	unstructured outdoor environments. That is because the system assesses the 
	traversability using an off-line learned model trained with specific 
	terrain types. For example, if a system is trained with terrain samples in a 
	specific season, the systems might not detect traversable regions in other 
	seasons \citep{Lee}.
	\par
	Unsupervised, or else self-supervised, learning approaches may be the solution 
	to this problem. They use on-line learning methods in order to exploit newly-
	acquired training data in making traversability predictions about unknown 
	terrain \citep{Kim}. That way the learned traversability concepts are 
	incrementally updated with new data only. That comes with the advantage that the 
	updated classifier is immediately available for navigation and that the memory 
	requirements for learning are reduced, compared to off-line methods.
	\par
	In the real world and its unstructured and dynamic surroundings such as vegetation 
	landscape and terrain, the perception of a mobile robot needs to be capable of 
	navigating this unknown environment by using sensor modalities \citep{Shabbir}.
	\\
	
	So, if autonomous mobile robots are to become more generally useful, they must 
	be able to adapt to new environments and learn from experience. To do so, they 
	need a way to store pertinent information about the environment, recall the 
	information at appropriate times, and reliably match stored information with 
	newly-sensed data. They also must be able to modify the stored information to 
	account for systematic changes in the environment \citep{Shneier}.
	\\
	
	Estimating the traversability of terrain in an unstructured outdoor 
	environment is a core functionality for autonomous robot navigation \citep{Kim}. 
	Nevertheless, the traversability of more complex terrain, such as 
	vegetation and sloping ground, is extremely difficult to characterize a priori. 
	It is difficult to find general rules which work for each vehicle's	capabilities
	and for a wide variety of terrain types such as trees, rocks, tall grass, logs, 
	and bushes. As a result, methods which provide traversability estimates 
	based on predefined terrain properties such as height, shape or colour 
	(\term{geometry-based} and \term{appearance-based} analyses) will be unlikely to 
	work reliably in unknown outdoor environments. That is why combining data 
	collected a priori together with the vehicle’s navigation experience is more 
	likely to work better for deciding terrain traversability (see more about 
	\term{hybrid} approaches in \citet{Papadakis}).
	\\
	
	Last but not least, traversability should be treated as an affordance and 
	not simply as a predefined property of different types of terrain \citep{Kim}. 
	That is because a large vehicle may be able to drive over small saplings that 
	would present an insurmountable obstacle to a smaller vehicle. A stair that is 
	traversable for a hexapod robot may not be traversable for a wheeled one. So, 
	when used here, \term{affordance} implies the complementarity of the robot and 
	the environment, the interaction between them \citep{Ugur}.
	\\
	
	In this thesis, we will tackle how an autonomous mobile robot can improve its 
	traversability estimation method in natural environments, meaning not 
	only on bare ground-like environment, but also on terrain containing vegetation. 
	On contrast, we will rule out high-risk applications where a single accident can 
	be fatal to the robot, like planetary or volcano exploration. We will concentrate 
	in everyday practical situations. We will determine how to introduce a learning 
	capability to the robot that will enable it to decide for itself the 
	traversability of the terrain around it, based on input from its sensors 
	and its experience of traveling over similar terrain in the past. We would also 
	like our robot to plan further ahead and avoid entering traps that prevent it 
	from reaching its goal.
	\\
	
	\chapter{Background}
	\label{sec:bg}
	
	\section{A few words}
	\label{sec:bg:intro}
	
	In order to have an autonomous robot improve its traversability 
	estimation we will need to address each sub-problem individually:
	
	\begin{enumerate}
		\item Traversability estimation algorithms that can be improved from 
		experience/examples.
		\item Methods for collecting the data needed by the algorithm above, from 
		the sensory input that is available to the robot. (The input we have does not 
		directly map to positive/negative decision).
		\item Navigation strategies. There might be an explicit goal to achieve, e.g.
		follow the fastest or easiest route to a target. Or it could be curiosity-driven 
		exploration, meaning the abstract need to learn a new environment.
	\end{enumerate}
	
	We will now present the state of the art in all three areas of research.
	\\
	
	\section{Learning traversability estimation algorithms}
	\label{sec:bg:trav}
	
	In order for an autonomous robot to be able to safely navigate, it is crucial 
	for it to be able to conclude on its own the terrain traversability 
	around it. Historically, most commonly, traversability analysis is 
	treated as a binary classification problem \citep{Papadakis}, i.e. distinguishing 
	traversable from non-traversable terrain. But later on, it became clear that 
	rough natural terrain is not easily partitioned into clear traversable and non-
	traversable classes. The need for finer classification was recognized. The new 
	idea was to either assign a continuous traversability score or classify 
	the terrain into the various classes that were commonly encountered within a 
	particular application. Many papers have been published regarding 
	traversability estimation approaches, and here we present some of the 
	most recent and most influential.
	\\
	
	This line of research starts with \citet{Lalonde} that segment local three-
	dimensional (\acronym{3d}) \term{point clouds} using a purely geometric 
	approach, for autonomous robot navigation purposes. A 
	\term{point cloud} is a set of data points in space, generally produced by 
	\acronym{3d} scanners. The approach used is a segmentation in three terrain 
	categories, based on scatter-ness, linear-ness, and surface-ness. That way 
	the authors are able to represent porous volumes such as grass and tree canopy, 
	capture thin objects like wires or tree branches, and capture solid objects 
	like ground surface, rocks or large trunks, respectively.
	\\	
	
	A different line of research starts with \citet{Pfaff} that decided to represent 
	the environment of a mobile robot with \term{elevation maps}, another geometric 
	approach. A \term{digital elevation map (\acronym{dem})} \citep{Kweon} is also 
	known as a \term{2\(\itfrac{1}{2}\)-dimensional representation} of the environment 
	\citep{Pfaff}. It is a two-dimensional (\acronym{2d}) array of terrain elevation 
	measurements. More concrete, it is a grid that stores in each cell the vertical 
	distance above or below the corresponding surface, the height of the territory. 
	\par 
	The representation of the environment with elevation maps, however, can 
	be problematic when a robot has to utilize these maps for navigation. For 
	example, when a mobile robot is located in front of a bridge, the underpass will 
	completely disappeared and the elevation map will show a non-traversable 
	object.
	\par
	\todo[size={{\scriptsize}}]{Explanation of representation and traversability of the area under the bridge}
	\citet{Pfaff} classify the cells of elevation maps into four classes: 
	parts of terrain seen from above, vertical structures, vertical gaps and 
	traversable areas. They also maintain a set of intervals per grid cell, which 
	are computed and updated upon incoming sensor data. The authors use this 
	classification for their extension to the elevation maps. The advantage here is 
	that they can deal with vertical structures like walls of buildings, but also 
	with overhanging structures like branches of trees or bridges. In order to 
	determine the class of a cell, they consider the variance of the height of all 
	measurements falling into this cell. If this value exceeds a certain threshold, 
	they identify it as a point that has not been observed from above. Then they 
	check whether the point set corresponding to a cell	contains gaps exceeding the 
	height of the robot. When a gap has been identified, they determine the	minimum 
	traversable elevation in this point set. So they only keep the height values for 
	the lowest surface in each cell. As a result, the area under the bridge, in the 
	previous example, will appear as a traversable surface, and the bridge will not 
	be represented.
	\\
	
	Yet, another approach works a little differently. The autonomous 
	vehicle has also to decide for itself the traversability of the terrain 
	around it. But it has no a priori knowledge of the kind of terrain it will 
	traverse, so it must learn as it goes along by observing the geometry and 
	appearance of the terrain. That is both proprioceptive and exteroceptive sensory 
	data processing \citep{Papadakis}. In a few words, \term{proprioceptive} analysis 
	is useful in learning while the vehicle traverses a given terrain, gathering data 
	with on-board sensors as it goes. On the other hand \term{extreroceptive} data 
	processing is divided in geometry-based and appearance-based analysis. 
	\par
	\citet{Shneier} follow a hybrid approach such as the above. They use a local 
	\term{occupancy grid} map that scrolls under the vehicle as the vehicle moves,
	and cells that scroll off the end of the map are forgotten. 
	\term{Occupancy grid} maps \citep{Moravec} are \acronym{2d} arrays depicting the 
	robot’s environment with regions classified as empty, occupied or unknown.
	\citet{Shneier} do not 
	use a global map and the previous known information is forgotten once the robot 
	moves away from that location. Considering distance above or below the ground, 
	color, texture, and contrast, they estimate each cell’s traversability. 
	This estimation of the cost of traversing regions is used to generate models of 
	terrain in order for the robot to learn from its own experience.
	\\
	
	Another hybrid approach is this of \citet{Kim}. They 
	developed a method that is based on autonomous training data collection which 
	exploits the robot’s experience in navigating its environment to train 
	classifiers without human intervention. The main idea is that image-
	data obtained in the past is associated with traversability labels 
	obtained in the present, the so called \term{on-line machine learning}. The 
	learning process produces a classifier which makes traversability 
	predictions for new terrain regions. Successes and failures of the navigation 
	provide positive and negative traversability labels for cells in a grid-
	based representation of the terrain surrounding the vehicle. Cells under the 
	robot footprint that can be driven over are traversable and therefore yield 
	positive training examples, while those that hinder the robot’s motion are non-
	traversable and result in negative examples.
	\\
	
	Later on, \citet{Suger} proposed a learning approach that uses a \acronym{2d} 
	occupancy grid map (like \citet{Shneier}), where each cell stores features that 
	provide information from the senors. Every sell is associated with at least one 
	feature vector that is computed from the \acronym{3d} point clouds (like 
	\citet{Lalonde}) that are mapped to the respective cell. The authors use the 
	features mentioned bellow (mostly geometrical, like \citet{Lalonde} and 
	\citet{Pfaff}) to distinguish different types of terrain as well as 
	traversability constraints of the robot. 
	\begin{enumerate}
		\item[$\bullet$] Maximum height difference and
		\item[$\bullet$] slope 
	\end{enumerate}
	reflect the ground-clearance of the robot as well as the motor power.
	\begin{enumerate}
		\item[$\bullet$] Roughness and
		\item[$\bullet$] remission values (meaning the the reflection or scattering 
		of light by a material) 
	\end{enumerate}
	help to distinguish concrete and vegetation types.
	\par
	In contrast with \citet{Kim}, they collect partially and only positive labeled 
	training data. And then they use existing strategies \citep{Denis, Elkan} to 
	learn a classifier from this kind of training data.
	\\
	
	Similarly to \citet{Kim}, \citet{Lee} employ a self-supervised on-line learning 
	approach. As the vehicle explores its environment, the classifier is trained 
	incrementally with autonomously labeled training samples. Their approach 
	determines whether unknown regions in front of a vehicle are drivable while the 
	vehicle is in motion and without human’s input. Their traversability detection 
	method is based on \term{incremental nonparametric Bayesian clustering 
	(\acronym{inbc})}. In probability theory and statistics, \term{Bayes' theorem} 
	(alternatively \term{Bayes' law} or \term{Bayes' rule}) describes the probability 
	of an event, based on prior knowledge of conditions that might be related to the 
	event. Many approaches have used it for traversability estimation. For example, 
	\citet{Suger} use a naive Bayes classifier \citep{Denis}, and \citet{Lalonde} use 
	Bayesian classification to label the incoming data.
	\\
	
	\todo[size={{\scriptsize}}]{MLS maps and Droeschel et al.}
	Several authors have considered the problem of \term{simultaneous localization 
	and mapping (\acronym{slam})} in an outdoor environment. Some tried to solve it 
	with elevation maps generated from \acronym{3d} range data acquired with a mobile 
	robot \citep{Pfaff}. But elevation maps only model a single surface, they lack the 
	ability to represent vertical structures or even multiple levels. \term{Multi-level 
	surface} maps (\term{\acronym{mls}} maps) \citep{Triebel}, on the other hand, store 
	multiple heights in each grid cell. This extension allows a mobile robot to model 
	environments with more than one surface, such as bridges, underpasses, buildings 
	or mines. 
	\par
	The approach of \citet{Pfaff} allows to deal with vertical and 
	overhanging objects in elevation maps. Despite their efforts, they still lack 
	the ability to represent multiple surfaces. For example, the robot can plan a 
	path under a bridge but not over it, as mentioned before.
	\par
	The attention had most often been focused on methodologies that access the 
	traversability characteristics before actually driving over the respective region 
	\citep{Papadakis}. But \citet{Droeschel} use a way for continuous mapping and 
	localization during mission, without the necessity to map the environment 
	beforehand or to stop for acquiring new \acronym{3d} scans and to process them. 
	Their representation consists of local maps (\term{multiresolution} maps as the 
	authors call them) and a global map (called \term{allocentric} map).
	\par
	Each local map is a robot-centered \acronym{3d} grid map. It has high resolution 
	in the vicinity of the robot and coarser resolutions with increasing distance 
	(hence its name multiresolution map). Each cell stores \acronym{3d} point 
	measurements (including height from ground) along with occupancy information. 
	Since the robot, hence the sensor too, is moving during acquisition of the data, 
	individual grid cells are stored in a circular buffer to allow for shifting 
	elements in constant time. So when the robot moves, the circular buffers are 
	shifted whenever necessary to maintain the egocentric property of the map.
	\par
	A forward-looking image alone may be insufficient for planning and navigation 
	\citep{Kweon}. Robots operating in rough terrain may require knowledge of terrain 
	that has been observed but is currently out of the sensor field of view such as 
	terrain under and behind the robot. And that is the main reason why global maps 
	are useful. 
	\par
	In this case \citep{Droeschel}, the global map is built from local multiresolution 
	maps acquired at different view poses of the robot. This is useful in order to 
	overcome pose errors and to localize the robot with respect to a fixed frame. 
	While traversing the environment, a local map is extended whenever the robot 
	explores previously unseen terrain and optimized when a \term{loop closure} has 
	been detected. The loop closure problem consists in detecting when the robot has 
	returned to a past location after having discovered new terrain for a while. The 
	authors localize towards this local map during mission to get the pose of the 
	robot in the global map. They assess the traversability of the terrain by 
	analyzing height differences in the global map and plan cost-optimal paths.
	\\
	
	Subsequently, \citet{Wigness} proposed another way to learn new behaviors quickly 
	in the field with no or minimal human supervision. They propose a methodology for 
	learning reward functions from human examples via visual perception. This means 
	that the agent learns how to simply assign costs to distinct terrain types, and 
	follows the trajectory with the minimum cost. This approach is more focused than 
	this of \citet{Suger} in following the optimum path, but less in experimenting 
	with traversability. It also insists on dynamic environments, while \citet{Suger} 
	interprets the characteristic of traversability to be static, and further assume 
	that dynamic objects are detected and removed in advance.
	\\
	
	\todo[size={{\scriptsize}}]{Hirose et al. presenting GONet (we have the code for this)}
	\citet{HiroseGonet} introduced a semi-supervised approach for traversability 
	estimation, called GONet. The core of the proposed approach are \term{Generative 
	Adversarial Networks (\acronym{gan}s)} \citep{Goodfellow}. \acronym{gan}s are a 
	framework for estimating generative models via an adversarial process. They 
	simultaneously train two models. A generative model, let's say \alleg{a team of 
	counterfeiters}, that captures the distribution of the training data and tries 
	to produce fake samples and use it without detection. And a discriminative model, 
	let's call it \alleg{police}, that tries to detect the fake images by estimating 
	the probability that a sample came from the training data rather than the 
	\alleg{counterfeiters}. Competition in this framework drives both teams to 
	improve their methods until the \alleg{counterfeits} are indistinguishable from 
	the genuine samples.
	\par
	There is no need for any \term{Markov chains} or approximate inferences during 
	either training or generation of samples \citep{Goodfellow}. A \term{Markov chain} 
	is a stochastic (or random) process describing a sequence of possible events in 
	which the probability of each event depends only on the state attained in the 
	previous event. It has actually many similarities with Bayes' theorem mentioned 
	above. Many authors use an extension of Markov chains, named \term{Markov Decision 
	Process (\acronym{mdp})}, to formulate the problem of autonomous navigation and 
	allow mobile robots to make decisions \citep{Wigness, Zhelo}. Others use 
	\term{Markov Random Field (\acronym{mrf})} \citep{Li} to i.e. enforce spatial 
	consistency in a map or the preference that neighboring points have the same label 
	\citep{Lalonde}.
	\par
	Returning to GONet \citep{HiroseGonet}, intuitively, it works comparing two images, 
	an input image and a generated one. The generated image is created with a 
	particular class of \acronym{gan}s trained on positive examples only. It is 
	similar to the input image and looks as if it came from the actual positive 
	examples. The GONet compares the input with the generated image to decide whether 
	the area seen through the input image is traversable. The main assumption of 
	the approach is that when the input indeed shows a traversable area, the 
	generated image would look very similar to it. But when the input depicts a non-
	traversable scenario, then the generated image would look different.
	\par
	The generated images not only look like	traversable areas, but also resemble the 
	input query. To the best of the authors knowledge, they were the first that used 
	this image manipulation technique for traversability estimation.
	\\\\
	
	
	\todo[size={{\scriptsize}}]{Corrected from notes}
	We have presented recent methods on how to conduct traversability estimation 
	models from data; noting that data needs to be labeled. We will now proceed to 
	present how this labeled data can be autonomously acquired and which sensors are 
	needed.
	\\
	
	\section{Data collection methods}
	\label{sec:bg:data}
	
	For a long time until in recent years, robots have long been used in industrial 
	environments. In industrial environments, robotic systems are pre-programmed 
	with repetitive assignments which lack the capability of autonomy and as such 
	operate on the basis of a structured approach \citep{Shabbir}. Such an environment
	cannot be adaptive for a mobile robot since it eliminates the need for autonomy. 
	As such, surviving and adapting in the real world is more complex for any robotic
	system in comparison to the industrial setting since the risk of failure, system 
	error, external factors, obstacles, corrupt data, human error and unrecognizable 
	environments is more prevalent.
	\\
	
	So, for unstructured environments, a way to collect training data is to obtain 
	them through a human operator that drives a safe trajectory that is similar to 
	the environment where the robot should later be able to reliable operate in. 
	This process for training data generation has the advantage that it is fairly 
	easy to execute.
	\par 
	One way to do that is to label the cells of the map that intersect with the 
	projection of the footprint of the robot as positive examples \citep{Suger}. 
	This has the drawback that the labeled data are only positive examples, leaving 
	tons of unlabeled data to learn from.
	\par
	A similar approach, inspired by the above, is to train the robot with many 
	positive images of traversable places and just a small set of negative images 
	depicting blocked and unsafe areas \citep{HiroseGonet}. The positive examples can 
	be collected easily by simply operating the robot through traversable spaces, 
	while obtaining negative examples is time consuming, costly, and potentially 
	dangerous. But small amounts of negative examples can improve traversability 
	estimation in comparison to using only positive data.
	\par 
	In a variation of this, optimal trajectory examples are collected \citep{Wigness} 
	in order to be used from a reward function and train the robot.
	\\
	
	Another way is to autonomously collect data without any human supervision 
	\citep{Kim, Lee}. The robot can image the terrain in front of it and store the 
	resulting image patches in a data pool \citep{Kim}. Then, each image patch is an 
	observation of a single cell in a grid-based terrain map. Initially all of this 
	data is unlabeled, because the robot has not yet interacted with the terrain, and 
	its traversability is unknown. Then the robot attempts to drive over the 
	terrain that it previously observed, thus discovering the traversability 
	properties of the environment.
	\\\\
	
	
	Autonomous driving in unstructured environments faces many challenges which do 
	not exist in structured environments \citep{Shabbir}. In unstructured environments, 
	object attributes needed for driving cannot be defined as priori. Information 
	concerning objects has to be gained through sensors even though these are normally 
	ambiguous and therefore introduce uncertainty and avail information that is 
	redundant.
	\par
	\todo[size={{\scriptsize}}]{Comparison of LIDAR and stereo camera sensory input}
	In environments where the ground is not flat or contains obstacles that are 
	not purely vertical, the basic approach of classifying based on the observed 
	obstacles from \acronym{2d} laser scanners can not be safely used anymore. 
	In these cases, \acronym{3d} range data, by i.e. stereo-cameras, radar or 
	\acronym{3d}-laser scanners, is necessary. A popular approach to collect data, 
	\todo[size={{\scriptsize}}]{All sensors are used to collect data. So how come LIDAR and stereo camera are more "estimation" than "data collection"?}
	either for the initial training or to use them for traversability estimation, is to use 
	\term{light detection and ranging (\acronym{lidar})} \citep{Suger, Lalonde} (also 
	called \term{ladar} \citep{Lalonde, Shneier}). \acronym{lidar} is a surveying 
	method that measures distance to a target by illuminating the target with pulsed 
	laser light and measuring the reflected pulses with a sensor. \acronym{lidar} 
	sensors use emitted light, so they work independent of the ambient light. Night 
	or day, clouds or sun, shadows or sunlight, they pretty much see the same in all 
	conditions. On the other hand they often have trouble sensing highly reflective 
	surfaces and transparent objects, such as mirrors and glass doors 
	\citep{HiroseGonet}.
	\par
	Other commonly used sensors are stereo cameras. They are really inexpensive, 
	especially compared to \acronym{lidar}. Because they use reflected light, they 
	can see an arbitrary distance in the daytime, as opposed to \acronym{lidar} 
	whose range of vision is limited. They also have higher resolution and are able 
	to see color, instead of just a grayscale. But they need illumination at night, 
	and headlights might not be enough. When it comes to geometric data, stereo 
	camera sensory might not be enough to detect all important features with the 
	reliability necessary for safe traversing.
	\par
	In occasions where sensors that can measure in all directions are needed, 
	hardware requirements are imposed. One can use a laser scanner that rotates 
	around a vertical axis \citep{Droeschel}. That way the sensor can measure in 
	all directions, except for a cylindrical blind spot around the vertical axis 
	centered on the robot.
	\\
	
	Another option is to not use geometric data, but instead concentrate on visual 
	data. Instead of using \acronym{lidar} \citep{Suger, Lalonde}, one can use 
	different sensors, like fisheye camera \citep{Hirose, HiroseGonet}, 
	to estimate whether a physical space is traversable or not. This kind of 
	approaches are mainly focused on obstacle detection and avoidance, even in 
	dynamic environments. But they are less interested in traversability 
	estimation for obstacles that may seem untraversable while in fact can be 
	easily driven over by a robot, like tall grass.
	\par
	More so, even without the introduction of uncertainty, sensors in themselves are 
	ambiguous \citep{Shabbir}. For example, a lemon and a soccer ball can look 
	similar from a certain perspective. In addition, a cup could be invisible in case 
	the cupboard is shut and it can be challenging to tell the difference between a 
	remote control and cell phone is they are both facing down. These factors are all
	contributive to the challenges of perceiving the state of the environment.
	\\
	
	A third choice is to use proprioceptive information, via on-board sensors such 
	as \term{inertial measurement unit (\acronym{imu})}, motor current, and bumper 
	switch \citep{Kim} or even wheel encoder data \citep{Lee} (like wheel odometry 
	measurements \citep{Droeschel}). \acronym{imu} is an electronic device using 
	\todo[size={{\scriptsize}}]{Fixed IMU definition}
	a combination of accelerometers and gyroscopes, sometimes also magnetometers. 
	It measures and reports a robot's specific force, angular rate, and the 
	\acronym{6d} robot pose (i.e. \acronym{3d} location and orientation).
	\par
	The use of the sensors above make is possible to assess the progress of the 
	robot automatically and estimate its motion \citep{Droeschel}. That way successes 
	and failures of the navigation provide positive and negative traversability 
	examples \citep{Kim}. This kind of approaches can make predictions about the 
	traversability of the terrain based on the robot's past experiences and 
	navigation sensor values.
	\\
	
	In some cases all three choices are used \citep{Kim, Shneier}. While geometric 
	data provide information about the traversability of the terrain, they are 
	not always sufficient to measure the affordance of traversability. For 
	example, a short (non-traversable) tree trunk and a patch of tall (traversable) 
	grass will result in a similar height. However, they differ in visual 
	appearance. 
	\par 
	Similarly a white vertical flat surface may be an impenetrable wall in one 
	environment whereas in another environment a similar surface may be a door that 
	can just be pushed to open \citep{Ugur}. But appearance data may not be enough 
	to distinguish the two cases mentioned above. So, a robot can be equipped with 
	stereo vision cameras which collect visual and geometric data from the 
	environment, but also with a bumper switch at the front of the vehicle that can 
	be used along with motor current sensors to recognize situations like getting 
	\todo[size={{\scriptsize}}]{a bumper switch can recognize an obstacle}
	stuck because of an obstacle or slipping, respectively \citep{Kim}.
	\par
	The reason why proprioceptive sensory may not be sufficient on their own is that 
	bumpers, for example, do not prevent robots from falling off edges and can fail 
	to detect small obstacles \citep{HiroseGonet}. That is why all geometric, 
	appearance and haptic information are useful.
	\\\\
	
	
	\todo[size={{\scriptsize}}]{Corrected from notes}
	But what is the strategy for deciding where to go next? Is there a specific goal 
	for the robot to reach? Should it find the best trajectory? Explore the 
	environment? A discussion of this topic is made in the following section.
	\\
	
	\section{Navigation strategies}
	\label{sec:bg:goals}
	
	A natural thing would be to let the robot learn about the traversability 
	of the environment, while another perspective would be to concentrate on more 
	preservative situations, such as go or no-go \citep{Hirose}. Even though the 
	former method allows the robot to autonomously learn a model of the environment,
	encourages exploration and consequently improves the learning and generalization 
	performance \citep{Zhelo}, the trial and error part of it involves a high risk to 
	damage the robot. The latter, on the other hand, has as main priority to prevent 
	robots from colliding with objects, injuring people, getting stuck in constrained 
	spaces, or falling over an edge. 
	\par
	A third approach would be to let the autonomous vehicle navigate from a defined 
	start point to a fixed goal	point \citep{Shneier, Zhelo}. That way the robot has 
	to build a model of the world around it and plan a path from the start to the 
	goal. A way to do that is to enable the robot to learn which regions to avoid 
	and which to seek out, in early runs, so that in later runs it can determine the 
	most efficient path \citep{Shneier}. In cases where there is no the knowledge of 
	the map of the robot's current environment, the designated goal location can be 
	acquired via cheap localization solutions, such as visible light localization, 
	Wi-Fi signal localization or \acronym{gps} \citep{Zhelo}.
	\\\\
	
	
	\todo[size={{\scriptsize}}]{Something that is missing from the strategies deciding where to go next}
	In our knowledge, most approaches concentrate on robot navigation in order to 
	make sure they do not collide with other objects and they follow a route to 
	reach a specific goal. There is not much work done on curiosity-driven 
	exploration, where there is no explicit goal, but the abstract need for the 
	robot to learn a new environment. The traversability estimation is most commonly 
	performed in order for the robot to move safely rather than explore the 
	environment and learn which areas are traversable and which are not.
	\\
	
	
		
	\section{Conclusions}
	\label{sec:bg:concl}
	
	What is missing in order to be able to achieve the promise in Chap 1
	
	
	\chapter{Core foreground}
	\label{sec:fg}
	
	Machine learning is a subset of artificial intelligence used to effectively 
	perform a specific task without using explicit instructions, but relying on models 
	and inference instead. The idea of using machine learning to control robots needs 
	humans to show the willingness to lose a certain measure of control \citep{Shabbir}. 
	This is seemingly counterintuitive in the beginning although the gain for doing 
	this is to allow the system to begin learning on its own.
	\\
	
	One of the most remarkable feats of the human visual system is how rapidly, 
	accurately and comprehensively it can recognize and understand the complex visual 
	world \citep{Socher}. The various types of tasks related to understanding what 
	we see in a visual scene is called \term{visual recognition}. In computer vision, 
	visual recognition has enjoyed some great success in recent years, particularly 
	in single object categorization. While recognizing isolated objects is a critical 
	component of visual recognition, a lot more is needed to be done to reach a 
	complete understanding of visual scenes.
	\\
	
	In this thesis, our first goal is for the robot to be able to autonomously 
	navigate in natural environments. So, the aim is on total scene segmentation 
	rather than object recognition.
	\par
	That is why we are going to use \term{neural networks} in order to attain 
	autonomous driving. Neural networks are computing systems vaguely inspired by 
	the biological neural network that constitute animal brains. A neural network 
	is a framework for many different machine learning algorithms to work together 
	and process complex data inputs. It learns to perform tasks by considering 
	examples, generally without being programmed with any task specific rules.
	\\

	Several real-world objects can share common attributes which allude to their 
	intended usage \citep{Shabbir}. By placing emphasis on these related object 
	attributes, the complexity of autonomous driving is lowered. For example, visual 
	data can be analyzed for the identification of few points which correspond to 
	positive locations at which a robot can maneuver. Furthermore, since movement
	features are similar across many objects, robots could be trained to identify 
	them. As a consequence, the state space that requires exploration in order to 
	move is significantly lowered.
	\\
	
	
	\chapter{Experimental Validation and Comparison}
	\label{sec:exp}
	
	\chapter{Conclusions and Future Work}
	\label{sec:concl}
	
	\appendix
	\chapter{Prototype / reference implementations}
	\label{sec:app}
	Many writers published their code open-source so that other researchers may 
	use their work. Here we give some \acronym{url}s and comments about the work 
	mentioned above in Section~\ref{sec:bg:trav}, Section~\ref{sec:bg:data} and 
	Chapter~\ref{sec:fg}. Some of them were actually used as our experimental basis.
	\\
	
	\begin{table}[h]
		\centering	
		\input{urls}
	\end{table}
	
	\renewcommand{\bibname}{References}
	\bibliography{ref}
	\bibliographystyle{unsrtnat}
\end{document}