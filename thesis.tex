\documentclass[12pt,a4paper]{report}
\usepackage[english]{babel}
\usepackage[utf8]{inputenc}

\usepackage[colorlinks]{hyperref}
\usepackage[colorinlistoftodos]{todonotes}

\usepackage[autostyle]{csquotes}
\usepackage[nottoc]{tocbibind}
\usepackage{indentfirst}

\usepackage[normalem]{ulem}
\usepackage{amsmath}

\newcommand{\etal}[1]{#1 et al.}
\newcommand{\defn}{\enquote}
\newcommand{\term}{\textit}
\newcommand{\acronym}{\MakeUppercase}
\newcommand{\itfrac}[2]{\frac{\textit{#1}}{\textit{#2}}}

\begin{document}
	{
		\hypersetup{linkcolor=black}
		\tableofcontents
	}
	
	\chapter{Introduction}
	\label{sec:intro}
	
	During the last years the focus of research for robotic applications evolved 
	from well structured indoor environments to unstructured outdoor environments. 
	With this expansion of interest, it is a crucial prerequisite to reliably 
	classify traversable ground in the environment, especially when it comes to 
	truly autonomous/self-supervised systems. This topic is typically referred to as 
	\term{traversability analysis} or \term{obstacle detection} \cite{Suger}. The 
	verb traverse is defined as \defn{to pass or move over, along, or through}. 
	Hence \term{traversability} refers to the affordance of being able to traverse 
	\cite{Ugur}. Failing on this task can cause great damage or restrict the robots 
	movement unnecessarily.
	\\
	
	So, \term{traversability} is the generic capability of a robotic ground 
	vehicle to navigate within environments of varying complexity, while ensuring 
	safety in terms of collisions or reaching unrecoverable states and achieving 
	goals in an optimal mode of operation \cite{Papadakis}. Occasionally other 
	terms such as \term{mobility} \cite{Lalonde}, \term{drivability} \cite{Droeschel}, 
	etc are used to describe the same thing.
	\\
	
	Although \term{traversability} is considered a fundamental capability for mobile 
	robots, in some cases it is limited to the problem of simple obstacle avoidance 
	\cite{Ugur}. When such approaches are used, the robot tries to avoid making any 
	physical contact with the environment, and heads only to open spaces. Its 
	response would be the same whether it encounters an impenetrable wall or a 
	balloon that can just be pushed aside without any damage. Therefore, methods 
	that can automatically learn the \term{traversability} affordances from the 
	robot’s interactions with the environment are valuable for robotics.
	\\
		
	In addition, there is the possibility that previously learned behavior is not 
	relevant, because the visual appearance and \term{traversability} of roads may 
	have changed due to various reasons \cite{Wigness}. That is probably why 
	\term{geometry-based} analysis is the direction followed by the majority of 
	\term{traversability analysis} methodologies in the past\cite{Papadakis}.
	\\
	
	Supervised learning approaches are unlikely to work reliably in unknown or 
	unstructured outdoor environments. That is because the system assesses the 
	\term{traversability} using an off-line learned model trained with specific 
	terrain types. For example, if a system is trained with terrain samples in a 
	specific season, the systems might not detect traversable regions in other 
	seasons \cite{Lee}.
	\par
	Unsupervised, or else self-supervised, learning approaches may be the solution 
	to this problem. They use on-line learning methods in order to exploit newly-
	acquired training data in making \term{traversability} predictions about unknown 
	terrain \cite{Kim}. That way the learned \term{traversability} concepts are 
	incrementally updated with new data only. That comes with the advantage that the 
	updated classifier is immediately available for navigation and that the memory 
	requirements for learning are reduced, compared to off-line methods.
	\\
	
	So, if autonomous mobile robots are to become more generally useful, they must 
	be able to adapt to new environments and learn from experience. To do so, they 
	need a way to store pertinent information about the environment, recall the 
	information at appropriate times, and reliably match stored information with 
	newly-sensed data. They also must be able to modify the stored information to 
	account for systematic changes in the environment \cite{Shneier}.
	\\
	
	Estimating the \term{traversability} of terrain in an unstructured outdoor 
	environment is a core functionality for autonomous robot navigation \cite{Kim}. 
	Nevertheless, the \term{traversability} of more complex terrain, such as 
	vegetation and sloping ground, is extremely difficult to characterize a priori. 
	It is difficult to find general rules which work for each vehicle's	capabilities
	and for a wide variety of terrain types such as trees, rocks, tall grass, logs, 
	and bushes. As a result, methods which provide \term{traversability} estimates 
	based on predefined terrain properties such as height, shape or colour 
	(\term{geometry-based} and \term{appearance-based} analyses) will be unlikely to 
	work reliably in unknown outdoor environments. That is why combining data 
	collected a priori together with the vehicle’s navigation experience is more 
	likely to work better for deciding terrain \term{traversability} (see more about 
	\term{hybrid} approaches in \cite{Papadakis}).
	\\
	
	Last but not least, \term{traversability} should be treated as an affordance and 
	not simply as a predefined property of different types of terrain \cite{Kim}. 
	That is because a large vehicle may be able to drive over small saplings that 
	would present an insurmountable obstacle to a smaller vehicle. A stair that is 
	traversable for a hexapod robot may not be traversable for a wheeled one. So, 
	when used here, \term{affordance} implies the complementarity of the robot and 
	the environment, the interaction between them \cite{Ugur}.
	\\
	
	In this thesis, we will tackle how an autonomous mobile robot can improve its 
	\term{traversability} estimation method in natural environments, meaning not 
	only on bare ground-like environment, but also on terrain containing vegetation. 
	On contrast, we will rule out high-risk applications where a single accident can 
	be fatal to the robot, like planetary or volcano exploration. We will concentrate 
	in everyday practical situations. We will determine how to introduce a learning 
	capability to the robot that will enable it to decide for itself the 
	\term{traversability} of the terrain around it, based on input from its sensors 
	and its experience of traveling over similar terrain in the past. We would also 
	like our robot to plan further ahead and avoid entering traps that prevent it 
	from reaching its goal.
	\\
	
	\chapter{Background}
	\label{sec:bg}
	
	\section{A few words}
	\label{sec:bg:intro}
	
	In order to have an autonomous robot improve its \term{traversability} 
	estimation we will need to address each sub-problem individually:
	
	\begin{enumerate}
		\item \term{Traversability} estimation algorithms that can be improved from 
		experience/examples.
		\item Methods for collecting the data needed by the algorithm above, from 
		the sensory input that is available to the robot. (The input we have does not 
		directly map to positive/negative decision).
		\item Goals of operation. There might be an explicit goal to achieve or it 
		could be curiosity-driven exploration, meaning the abstract need to learn a 
		new environment.
	\end{enumerate}
	
	We will now present the state of the art in all three areas of research.
	\\
	
	\section{Learning traversability estimation algorithms}
	\label{sec:bg:trav}
	
	In order for an autonomous robot to be able to safely navigate, it is crucial 
	for it to be able to conclude on its own the terrain \term{traversability} 
	around it. Historically \cite{Papadakis}, most commonly, \term{traversability} 
	analysis is treated as a binary classification problem, i.e. distinguishing 
	traversable from non-traversable terrain. But later on, it became clear that 
	rough natural terrain is not easily partitioned into clear traversable and non-
	traversable classes. The need for finer classification was recognized. The new 
	idea was to either assign a continuous \term{traversability} score or classify 
	the terrain into the various classes that were commonly encountered within a 
	particular application. Many papers have been published regarding 
	\term{traversability} estimation approaches, and here we present some of the 
	most recent and most influential.
	\\
	
	This line of research starts with \etal{Lalonde} that segment local three-
	dimensional (\acronym{3d}) \term{point clouds} using a purely geometric 
	approach \cite{Lalonde}, for autonomous robot navigation purposes. A 
	\term{point cloud} is a set of data points in space, generally produced by 
	\acronym{3d} scanners. The approach used is a segmentation in three terrain 
	categories, based on scatter-ness, linear-ness, and surface-ness. That way 
	the authors are able to represent porous volumes such as grass and tree canopy, 
	capture thin objects like wires or tree branches, and capture solid objects 
	like ground surface, rocks or large trunks, respectively.
	\\	
	
	A different line of research starts with \etal{Pfaff} that decided to represent 
	the environment of a mobile robot with \term{elevation maps}, another geometric 
	approach. A \term{digital elevation map} (\acronym{dem}) is also known as a
	\term{2\(\itfrac{1}{2}\)-dimensional representation} of the environment 
	\cite{Pfaff}. It is a two-dimensional (\acronym{2d}) array of terrain elevation 
	measurements. More concrete, it is a grid that stores in each cell the vertical 
	distance above or below the corresponding surface (additional information about 
	\acronym{dem} can be found in \cite{Kweon}). 
	\par 
	The representation of the environment with \term{elevation maps}, however, can 
	be problematic when a robot has to utilize these maps for navigation. For 
	example, when a mobile robot is located in front of a bridge, the underpass will 
	completely disappeared and the \term{elevation map} will show a non-traversable 
	object.
	\par
	The authors in \cite{Pfaff} classify the cells of \term{elevation maps} into 
	parts of terrain seen from above, vertical objects,	overhanging objects (such as 
	branches of trees or bridges) and traversable areas. That way, they manage to 
	only keep the height values for the lowest surface in each cell. As a result, 
	the area under the bridge, in the previous example, will appear as a traversable 
	surface.
	\\
	
	Yet, another approach is kind of a mixture of the previous ones. The autonomous 
	vehicle has also to decide for itself the \term{traversability} of the terrain 
	around it. But it has no a priori knowledge of the kind of terrain it will 
	traverse, so it must learn as it goes along by observing the geometry and 
	appearance of the terrain. That is both proprioceptive and exteroceptive sensory 
	data processing (more about them can be found in \cite{Papadakis}). In a few 
	words, proprioceptive analysis is useful in learning while the vehicle traverses 
	a given terrain, gathering data with on-board sensors as it goes. On the other 
	hand extreroceptive data processing is divided in geometry-based and appearance-
	based analysis. 
	\par
	\etal{Shneier} follow an approach such as the above. They use a local 
	\term{occupancy grid} map that scrolls under the vehicle as the vehicle moves,
	and cells that scroll off the end of the map are forgotten \cite{Shneier}. 
	\term{Occupancy grid} maps are \acronym{2d} arrays depicting the robot’s 
	environment with regions classified as empty, occupied or unknown (for more 
	details about those see \cite{Moravec}). In \cite{Shneier} the authors do not 
	use a global map and the previous known information is forgotten once the robot 
	moves away from that location. Considering distance above or below the ground, 
	color, texture, and contrast, they estimate each cell’s \term{traversability}. 
	This estimation of the cost of traversing regions is used to generate models of 
	terrain in order for the robot to learn from its own experience.
	\\
	
	Another \term{hybrid} approach except \cite{Shneier} is this of \etal{Kim}. They 
	developed a method that is based on autonomous training data collection which 
	exploits the robot’s experience in navigating its environment to train 
	classifiers without human intervention \cite{Kim}. The main idea is that image-
	data obtained in the past is associated with \term{traversability} labels 
	obtained in the present, the so called \term{on-line machine learning}. The 
	learning process produces a classifier which makes \term{traversability} 
	predictions for new terrain regions. Successes and failures of the navigation 
	provide positive and negative \term{traversability} labels for cells in a grid-
	based representation of the terrain surrounding the vehicle. Cells under the 
	robot footprint that can be driven over are traversable and therefore yield 
	positive training examples, while those that hinder the robot’s motion are non-
	traversable and result in negative examples.
	\\
	
	Later on, \etal{Suger} proposed a learning approach that uses a \acronym{2d} 
	\term{occupancy grid} map (like in \cite{Shneier}), where each cell stores 
	features that provide information from the senors \cite{Suger}. Every sell is 
	associated with at least one feature vector that is computed from the \acronym{3d} 
	\term{point clouds} (like in \cite{Lalonde}) that are mapped to the respective 
	cell. The authors use the features mentioned bellow (mostly geometrical, like in 
	\cite{Lalonde, Pfaff}) to distinguish different types of terrain as well as 
	\term{traversability} constraints of the robot. 
	\begin{enumerate}
		\item[$\bullet$] Maximum height difference and
		\item[$\bullet$] slope 
	\end{enumerate}
	reflect the ground-clearance of the robot as well as the motor power.
	\begin{enumerate}
		\item[$\bullet$] Roughness and
		\item[$\bullet$] remission values (meaning the the reflection or scattering 
		of light by a material) 
	\end{enumerate}
	help to distinguish concrete and vegetation types.
	\par
	In contrast with \cite{Kim}, they collect partially and only positive labeled 
	training data. And then they use existing strategies \cite{Denis, Elkan} to 
	learn a classifier from this kind of training data.
	\\
	
	Similarly to \cite{Kim}, \etal{Lee} employ a self-supervised on-line learning 
	approach. As the vehicle explores its environment, the classifier is trained 
	incrementally with autonomously labeled training samples \cite{Lee}. Their 
	approach determines whether unknown regions in front of a vehicle are drivable 
	while the vehicle is in motion and without human’s input. Their 
	\term{traversability} detection method is based on incremental nonparametric 
	Bayesian clustering (\acronym{inbc}). In probability theory and statistics, 
	Bayes' theorem (alternatively Bayes' law or Bayes' rule) describes the 
	probability of an event, based on prior knowledge of conditions that might be 
	related to the event. Many approaches have used it for \term{traversability} 
	estimation. For example \cite{Suger} uses a naive Bayes classifier \cite{Denis}, 
	and \cite{Lalonde} uses Bayesian classification to label the incoming data.
	\\
	
	Several authors have considered the problem of simultaneous localization and 
	mapping (\acronym{slam}) in an outdoor environment. Some tried to solve it with 
	\term{elevation maps} generated from \acronym{3d} range data acquired with a 
	mobile robot \cite{Pfaff}. But \term{elevation maps} only model a single surface, 
	they lack the ability to represent vertical structures or even multiple levels.
	\term{Multi-level surface maps} (\acronym{mls} maps), on the other hand, store 
	multiple heights in each grid cell. This extension allows a mobile robot to model 
	environments with more than on surface, such as bridges, underpasses, buildings 
	or mines (you can find out more about \acronym{mls} maps in \cite{Triebel}). The 
	approach of \cite{Pfaff} allows to deal with vertical and overhanging objects in 
	\term{elevation maps}, by proposing a method for detecting loop closures, but it 
	still lacks the ability to represent multiple surfaces. For example, the robot 
	can plan a path under a bridge but not over it.
	\par
	\etal{Droeschel} use a different way for continuous mapping and localization 
	\cite{Droeschel}. \sout{Their representation consists of multiple robot-centered 
	\acronym{3d} grid maps with different resolutions.} For instance, distance 
	measurements are accumulated in a \acronym{3d} map with increasing cell sizes 
	from the robot center. \sout{The height from ground or the occupancy information are 
	stored in separate \acronym{3d} maps, and so forth.} Since the robot, hence the 
	sensor too, is moving during acquisition of the data, individual grid cells are 
	stored in a circular buffer to allow for shifting elements in constant time. So 
	when the robot moves, the circular buffers are shifted whenever necessary to 
	maintain the egocentric property of the map.
	
	\todo[size={{\scriptsize}}]{Abstract, chapters 4 and 6}
	!!! Multi-resolution and allocentric map !!!
	
	Fully continuous mapping and
	localization during mission, without the necessity to map the environment beforehand
	or to stop for acquiring new 3D scans and to process them.
	
	The authors track the pose hypothesis by alternating the prediction of the robot 
	movement given the filter result and alignment of the current local multiresolution 
	map towards the allocentric map of the environment.
	\\
	
	Subsequently, \etal{Wigness} proposed another way to learn new behaviors quickly 
	in the field with no or minimal human supervision. They propose a methodology 
	for learning reward functions from human examples via visual perception 
	\cite{Wigness}. This means that the agent learns how to simply assign costs to 
	distinct terrain types, and follows the trajectory with the minimum cost. This 
	approach is more focused than \cite{Suger} in following the optimum path, but 
	less in experimenting with \term{traversability}. It also insists on dynamic 
	environments, while \cite{Suger} interprets the characteristic of 
	\term{traversability} to be static, and further assume that dynamic objects are 
	detected and removed in advance.
	\\\\
	
	
	Now that we know some important and a lot of recent projects on the 
	\term{traversability} estimation matter, we need to find out how to gather all 
	the data needed. In the next section, we present methods for collecting the data 
	required by the algorithms above and sensors needed to collect them.
	\\
	
	\section{Data collection methods}
	\label{sec:bg:data}
	
	A way to generate and collect training data is to obtain them through a human 
	operator that drives a safe trajectory that is similar to the environment where 
	the robot should later be able to reliable operate in. This process for training 
	data generation has the advantage that it is fairly easy to execute.
	\par 
	An example of this is \cite{Suger} that label the cells of the map that 
	intersect with the projection of the footprint of the robot as positive examples. 
	This has the drawback that the labeled data are only positive examples, leaving 
	tons of unlabeled data to learn from.
	\par 
	In a variation of this, optimal trajectory examples are collected \cite{Wigness} 
	in order to be used from a reward function and train the robot.
	\\
	
	Another way is to autonomously collect data without any human supervision 
	\cite{Kim, Lee}. In \cite{Kim} the robot images the terrain in front of it and 
	stores the resulting image patches in a data pool. Each image patch is an 
	observation of a single cell in a grid-based terrain map. Initially all of this 
	data is unlabeled, because the robot has not yet interacted with the terrain, and 
	its \term{traversability} is unknown. Then the robot attempts to drive over the 
	terrain that it previously observed, thus discovering the \term{traversability} 
	properties of the environment.
	\\\\
	
	In environments where the ground is not flat or contains obstacles that are 
	not purely vertical, the basic approach of classifying based on the observed 
	obstacles from \acronym{2d} laser scanners can not be safely used anymore. 
	In these cases, \acronym{3d} range data, e.g. \acronym{3d} \acronym{lidar} 
	data, is necessary \cite{Suger, Lalonde}. \acronym{lidar} (called \term{ladar} 
	in \cite{Lalonde, Shneier}) is an acronym used for \term{light detection and 
	ranging}). It is a surveying method that measures distance to a target by 
	illuminating the target with pulsed laser light and measuring the reflected 
	pulses with a sensor. Differences in laser return times and wavelengths that 
	can then be used to make digital \acronym{3d} representations of the target.
	\\
	
	Others do not use geometric data, but instead concentrate on visual data, 
	\term{appearance-based} methodologies as they are called from \cite{Papadakis}. 
	Instead of using \acronym{lidar} \cite{Lalonde, Suger}, they use different 
	sensors, like onboard and off-the-shelf fisheye camera \cite{Hirose}, to 
	estimate whether a physical space is traversable or not. This kind of 
	approaches are mainly focused on obstacle detection and avoidance, and less 
	on \term{traversability} estimation for obstacles that may seem untraversable 
	while in fact can be easily driven over by a robot, like tall grass.
	\\
	
	A third choice is to use haptic information, via on-board sensors such as 
	\acronym{imu} (acronym for inertial measurement unit), motor current, and bumper 
	switch \cite{Kim} or even wheel encoder data \cite{Lee} (like wheel odometry 
	measurements \cite{Droeschel}). \acronym{imu} is an electronic device that 
	measures and reports a robot's specific force, angular rate, and sometimes the 
	magnetic field surrounding it, using a combination of accelerometers and 
	gyroscopes, sometimes also magnetometers. 
	\par
	The use of the sensors above make is possible to assess the progress of the 
	robot automatically and estimate its motion \cite{Droeschel}. That way successes 
	and failures of the navigation provide positive and negative \term{traversability} 
	examples \cite{Kim}. This kind of approaches can make predictions about the 
	\term{traversability} of the terrain based on the robot's past experiences and 
	navigation sensor values.
	\\
	
	In some cases all three choices are used \cite{Kim}. While geometric data 
	provide information about the \term{traversability} of the terrain, they are 
	not always sufficient to measure the affordance of \term{traversability}. For 
	example, a tall (nontraversable) tree trunk and a patch of tall (traversable) 
	grass will result in a similar height. However, they differ in visual 
	appearance. 
	\par 
	Similarly a white vertical flat surface may be an impenetrable wall in one 
	environment whereas in another environment a similar surface may be a door that 
	can just be pushed to open \cite{Ugur}. But appearance data may not be enough 
	to distinguish the two cases mentioned above. So, a robot can be equipped with 
	stereo vision cameras which collect visual and geometric data from the 
	environment, but also with a bumper switch at the front of the vehicle that can 
	be used along with motor current sensors to recognize situations like getting 
	stuck or slipping \cite{Kim}.
	\\\\
	
	
	In some cases, a forward-looking image alone may be insufficient for planning and 
	navigation \cite{Kweon}. Robots operating in rough terrain may require knowledge 
	of terrain that has been observed but is currently out of the sensor field of 
	view such as terrain under and behind the robot. To resolve this problem one can 
	use a laser scanner that rotates around a vertical axis \cite{Droeschel}. That 
	way the sensor can measure in all directions, except for a cylindrical blind spot 
	around the vertical axis centered on the robot.
	\\\\
	
	
	But why do we want to collect data and estimate the terrain \term{traversability}?
	What is the goal to achieve? Do we want the robot to reach a specific goal? Find 
	the best trajectory? Explore the environment? A discussion of this topic is made 
	in the following section.
	\\
	
	\section{Goals of operation}
	\label{sec:bg:goals}
	
	A natural thing would be to let the robot learn about the \term{traversability} 
	of the environment, while another perspective would be to concentrate on more 
	preservative situations, such as go or no-go \cite{Hirose}. Even though the 
	former method allows the robot to autonomously learn a model of the environment,
	encourages exploration and consequently improves the learning and generalization 
	performance \cite{Zhelo}, the trial and error part of it involves a high risk to 
	damage the robot. The latter, on the other hand, has as main priority to prevent 
	robots from colliding with objects, injuring people, getting stuck in constrained 
	spaces, or falling over an edge. 
	\par
	A third approach would be to let the autonomous vehicle navigate from a defined 
	start point to a fixed goal	point \cite{Shneier, Zhelo}. That way the robot has 
	to build a model of the world around it and plan a path from the start to the 
	goal. A way to do that is to enable the robot to learn which regions to avoid 
	and which to seek out, in early runs, so that in later runs it can determine the 
	most efficient path \cite{Shneier}. In cases where there is no the knowledge of 
	the map of the robot's current environment, the designated goal location can be 
	acquired via cheap localization solutions, such as visible light localization or 
	Wi-Fi signal localization \cite{Zhelo}.
	\\
	
	\section{Prototype implementations / reference implementation}
	\label{sec:bg:code}

	Here we give some \acronym{url}s and comments about the work above. Some of them 
	might actually be used as our experimental basis.
	\\
		
	\begin{table}[h]
		\centering	
		\input{urls}
	\end{table} 
		
	\section{Conclusions}
	\label{sec:bg:concl}
	
	What is missing in order to be able to achieve the promise in Chap 1
	
	
	\chapter{Core foreground}
	\label{sec:fg}
	
	\chapter{Experimental Validation and Comparison}
	\label{sec:exp}
	
	\chapter{Conclusions and Future Work}
	\label{sec:concl}
	
	\renewcommand{\bibname}{References}
	\bibliography{ref}
	\bibliographystyle{ieeetr}
\end{document}